{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8196409e",
   "metadata": {},
   "source": [
    "# egoEMOTION Dataset Exploration\n",
    "\n",
    "*egoEMOTION* is the first dataset that couples egocentric visual and physiological signals with dense self-reports of emotion and personality across controlled and real-world scenarios. Participants completed emotion-elicitation tasks and naturalistic activities while self-reporting their affective state using the Circumplex Model and Mikels’ Wheel as well as their personality via the Big Five model.\n",
    "\n",
    "In this notebook, we (i) load a selected participant’s data, (ii) compute quick summary statistics to sanity-check shapes, sampling rates, and missing values, then (iii) visualize the sensor streams and task segments, (iv) inspect the corresponding POV/webcam video frames alongside eye-tracking frames, and finally (v) review the participant’s self-report annotations for each activity.\n",
    "\n",
    "Please refer to the `README.md` file in the [egoEMOTION](https://github.com/eth-siplab/egoEMOTION) repository for detailed information on each sensor stream (i.e., channel definition, sampling frequency, ...).\n",
    "\n",
    "Important: Please change the variable \"original_data_path\" to the path where you saved the downloaded egoEMOTION dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570628ee",
   "metadata": {},
   "source": "## Import relevant libraries"
  },
  {
   "cell_type": "code",
   "id": "049d48d8",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import cv2\n",
    "import json\n",
    "import subprocess\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define participant and paths",
   "id": "51a660f33688bc2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "participant = \"037\"\n",
    "original_data_path = 'INSERT_PATH_TO_DOWNLOADED_DATASET'"
   ],
   "id": "47eb548d9ebee3a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b97a3794",
   "metadata": {},
   "source": [
    "## Load files from participant"
   ]
  },
  {
   "cell_type": "code",
   "id": "5727c2d6",
   "metadata": {},
   "source": [
    "sensor_streams = [\n",
    "    \"ecg_90fps\", \"eda_90fps\", \"et\",\n",
    "    \"gaze_90fps\", \"imu_right_90fps\",\n",
    "    \"intensity_90fps\",\n",
    "    \"ppg_ear_90fps\", \"ppg_nose_90fps\",\n",
    "    \"pupils_90fps\", \"rr_90fps\", \n",
    "    \"gaze\", \"pupils\", \n",
    "    \"intensity\" \n",
    "]\n",
    "video_streams = [\"pov\", \"webcam\"]\n",
    "\n",
    "def load_npy(path: str):\n",
    "    \"\"\"Robust .npy loader: returns ndarray OR python object (dict) if stored that way.\"\"\"\n",
    "    obj = np.load(path, allow_pickle=True)\n",
    "    # dict saved via np.save loads as 0-d object array\n",
    "    if isinstance(obj, np.ndarray) and obj.dtype == object and obj.shape == ():\n",
    "        try:\n",
    "            obj = obj.item()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return obj\n",
    "\n",
    "sensor_dict: dict[str, object] = {}\n",
    "video_dict: dict[str, str] = {}\n",
    "\n",
    "# --- Sensors (.npy) ---\n",
    "for stream in sensor_streams:\n",
    "    path = os.path.join(original_data_path, participant, f\"{stream}.npy\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] missing sensor stream: {path}\")\n",
    "        continue\n",
    "    sensor_dict[stream] = load_npy(path)\n",
    "\n",
    "# --- Videos (.mp4) ---\n",
    "for stream in video_streams:\n",
    "    path = os.path.join(original_data_path, participant, f\"{stream}.mp4\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] missing video stream: {path}\")\n",
    "        continue\n",
    "    video_dict[stream] = path\n",
    "\n",
    "# --- Task Times (.npy) ---\n",
    "task_times = load_npy(os.path.join(original_data_path,'task_times.npy'))\n",
    "\n",
    "# --- Self-reports (valence, arousal, dominance, discrete emotions) --- #\n",
    "session_A_path = os.path.join(original_data_path, participant, f\"Session_A_{participant}.csv\")\n",
    "session_B_path = os.path.join(original_data_path, participant, f\"Session_B_{participant}.csv\")\n",
    "\n",
    "session_A_df = pd.read_csv(session_A_path) if os.path.exists(session_A_path) else None\n",
    "if session_A_df is None:\n",
    "    print(f\"[WARN] missing session CSV: {session_A_path}\")\n",
    "\n",
    "session_B_df = pd.read_csv(session_B_path) if os.path.exists(session_B_path) else None\n",
    "if session_B_df is None:\n",
    "    print(f\"[WARN] missing session CSV: {session_B_path}\")\n",
    "\n",
    "# --- Personality Questionnaire --- #\n",
    "personality_df = pd.read_csv(os.path.join(original_data_path, \"personality_questionnaire_results.csv\"), sep=';')\n",
    "if personality_df is None:\n",
    "    print(f\"[WARN] missing session CSV: {session_A_path}\")\n",
    "\n",
    "\n",
    "print(\"Loaded sensors:\", list(sensor_dict.keys()))\n",
    "print(\"Found videos:\", list(video_dict.keys()))\n",
    "print(\"Task times\", list(task_times.keys()))\n",
    "print(\"Session A loaded:\", session_A_df is not None)\n",
    "print(\"Session B loaded:\", session_B_df is not None)\n",
    "print(\"Personality questionnaire loaded:\", personality_df is not None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6fb68b00",
   "metadata": {},
   "source": [
    "## Synchronization checks"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ce484e4",
   "metadata": {},
   "source": [
    "def probe_video(path):\n",
    "    # ---------- Best: ffprobe ----------\n",
    "    try:\n",
    "        cmd = [\n",
    "            \"ffprobe\", \"-v\", \"error\",\n",
    "            \"-print_format\", \"json\",\n",
    "            \"-show_format\",\n",
    "            \"-show_streams\",\n",
    "            path\n",
    "        ]\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT).decode(\"utf-8\")\n",
    "        meta = json.loads(out)\n",
    "\n",
    "        # pick the first video stream\n",
    "        vstreams = [s for s in meta.get(\"streams\", []) if s.get(\"codec_type\") == \"video\"]\n",
    "        vs = vstreams[0] if vstreams else {}\n",
    "\n",
    "        # duration: container format duration is usually most reliable\n",
    "        dur = meta.get(\"format\", {}).get(\"duration\", None)\n",
    "        dur = float(dur) if dur is not None else None\n",
    "\n",
    "        # fps: try avg_frame_rate first\n",
    "        def _parse_rate(r):\n",
    "            if not r or r == \"0/0\": return None\n",
    "            num, den = r.split(\"/\")\n",
    "            return float(num) / float(den) if float(den) != 0 else None\n",
    "\n",
    "        fps = _parse_rate(vs.get(\"avg_frame_rate\")) or _parse_rate(vs.get(\"r_frame_rate\"))\n",
    "\n",
    "        # nb_frames might be missing for some files\n",
    "        nb_frames = vs.get(\"nb_frames\", None)\n",
    "        nb_frames = int(nb_frames) if nb_frames is not None and str(nb_frames).isdigit() else None\n",
    "\n",
    "        w = vs.get(\"width\", None)\n",
    "        h = vs.get(\"height\", None)\n",
    "\n",
    "        return {\n",
    "            \"path\": path,\n",
    "            \"readable\": True,\n",
    "            \"fps\": fps,\n",
    "            \"n_frames\": nb_frames,\n",
    "            \"duration_s\": dur,\n",
    "            \"size\": (w, h),\n",
    "            \"source\": \"ffprobe\",\n",
    "        }\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ---------- Fallback: OpenCV (less reliable) ----------\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        return {\"path\": path, \"readable\": False, \"source\": \"opencv\"}\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    ok, _ = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    dur = (n_frames / fps) if fps and fps > 0 and n_frames > 0 else None\n",
    "    return {\n",
    "        \"path\": path,\n",
    "        \"readable\": bool(ok),\n",
    "        \"fps\": float(fps) if fps and fps > 0 else None,\n",
    "        \"n_frames\": n_frames if n_frames > 0 else None,\n",
    "        \"duration_s\": float(dur) if dur is not None else None,\n",
    "        \"size\": (w, h),\n",
    "        \"source\": \"opencv\",\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------- SENSOR QC --------------------\n",
    "\n",
    "def _unwrap_stream(x):\n",
    "    \"\"\"Handle dict-like saved npy; otherwise return as-is.\"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for k in (\"data\", \"values\", \"samples\", \"signal\", \"x\", \"frames\"):\n",
    "            if k in x:\n",
    "                return x[k]\n",
    "    return x\n",
    "\n",
    "def _to_2d(arr):\n",
    "    \"\"\"(N,) -> (N,1); (N,...) -> (N,C)\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 1:\n",
    "        return arr.reshape(-1, 1)\n",
    "    if arr.ndim == 2:\n",
    "        return arr\n",
    "    # e.g., ET frames (N,H,W) -> keep as 3D elsewhere; caller decides\n",
    "    return arr\n",
    "\n",
    "def infer_fs(name, default_fs=None):\n",
    "    \"\"\"Simple inference. Extend if you have known native rates.\"\"\"\n",
    "    n = name.lower()\n",
    "    if \"_90fps\" in n or n == \"et\":\n",
    "        return 90.0\n",
    "    return default_fs\n",
    "\n",
    "def sensor_summary(stream_dict, default_fs=None):\n",
    "    \"\"\"\n",
    "    Prints fs + key QC stats per stream:\n",
    "    - shape\n",
    "    - n_samples, n_channels (where applicable)\n",
    "    - inferred duration if fs known\n",
    "    - NaN/Inf fraction\n",
    "    \"\"\"\n",
    "    print(\"\\n=== SENSOR STREAM QC ===\")\n",
    "    for name in sorted(stream_dict.keys()):\n",
    "        raw = stream_dict[name]\n",
    "        raw = _unwrap_stream(raw)\n",
    "        arr = np.asarray(raw)\n",
    "\n",
    "        fs = infer_fs(name, default_fs=default_fs)\n",
    "\n",
    "        # ET / image-like (N,H,W)\n",
    "        if arr.ndim == 3:\n",
    "            N, H, W = arr.shape\n",
    "            nan_frac = float(np.isnan(arr).mean()) if np.issubdtype(arr.dtype, np.floating) else 0.0\n",
    "            inf_frac = float(np.isinf(arr).mean()) if np.issubdtype(arr.dtype, np.floating) else 0.0\n",
    "            dur = (N / fs) if fs else None\n",
    "            print(f\"- {name:18s} shape={arr.shape} dtype={arr.dtype}  fs≈{fs}  dur≈{dur:.2f}s  NaN={nan_frac:.4f} Inf={inf_frac:.4f}\")\n",
    "            continue\n",
    "\n",
    "        # Regular signals (N,C)\n",
    "        arr2 = _to_2d(arr)\n",
    "        if arr2.ndim != 2:\n",
    "            print(f\"- {name:18s} shape={arr.shape} dtype={arr.dtype}  (unhandled dims)\")\n",
    "            continue\n",
    "\n",
    "        N, C = arr2.shape\n",
    "        if np.issubdtype(arr2.dtype, np.floating):\n",
    "            nan_frac = float(np.isnan(arr2).mean())\n",
    "            inf_frac = float(np.isinf(arr2).mean())\n",
    "        else:\n",
    "            nan_frac = inf_frac = 0.0\n",
    "\n",
    "        dur = (N / fs) if fs else None\n",
    "        dur_str = f\"{dur:.2f}s\" if dur is not None else \"n/a\"\n",
    "        fs_str = f\"{fs:.3f}Hz\" if fs is not None else \"n/a\"\n",
    "        print(f\"- {name:18s} shape={arr2.shape!s:12s} dtype={arr2.dtype}  fs≈{fs_str:>8s}  dur≈{dur_str:>8s}  NaN={nan_frac:.1f} Inf={inf_frac:.1f}\")\n",
    "\n",
    "# -------------------- FULL QC REPORT --------------------\n",
    "\n",
    "def qc_report(participant_dir, sensor_dict, video_names=(\"pov.mp4\", \"webcam.mp4\"), default_sensor_fs=None):\n",
    "    # Sensors\n",
    "    sensor_summary(sensor_dict, default_fs=default_sensor_fs)\n",
    "\n",
    "    # Videos\n",
    "    print(\"\\n=== VIDEO QC ===\")\n",
    "    for vid in video_names:\n",
    "        vp = os.path.join(participant_dir, vid)\n",
    "        if not os.path.exists(vp):\n",
    "            print(f\"- {vid:10s} MISSING\")\n",
    "            continue\n",
    "        info = probe_video(vp)\n",
    "        print(f\"- {vid:10s} {info}\")\n",
    "\n",
    "    # Compare ET duration to videos\n",
    "    if \"et\" in sensor_dict:\n",
    "        et = np.asarray(_unwrap_stream(sensor_dict[\"et\"]))\n",
    "        if et.ndim == 3:\n",
    "            et_dur = et.shape[0] / 90.0\n",
    "            print(f\"\\n=== ALIGNMENT QUICK CHECK ===\")\n",
    "            print(f\"ET duration: {et_dur:.2f}s\")\n",
    "            for vid in video_names:\n",
    "                vp = os.path.join(participant_dir, vid)\n",
    "                if not os.path.exists(vp):\n",
    "                    continue\n",
    "                info = probe_video(vp)\n",
    "                if info.get(\"duration_s\") is not None:\n",
    "                    diff = info[\"duration_s\"] - et_dur\n",
    "                    print(f\"{vid}: duration {info['duration_s']:.2f}s  (video - ET = {diff:.2f}s)\")\n",
    "\n",
    "qc_report(os.path.join(original_data_path, participant), sensor_dict, default_sensor_fs=None)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8bcbaac1",
   "metadata": {},
   "source": [
    "## Summary statistics per task"
   ]
  },
  {
   "cell_type": "code",
   "id": "477c9ad0",
   "metadata": {},
   "source": [
    "# ---- config ----\n",
    "EMO_COLS = [\"Amused\",\"Content\",\"Excited\",\"Awe\",\"Neutral\",\"Fear\",\"Sad\",\"Disgust\",\"Anger\"]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    # \"TryNotToLaugh\" == \"try_not_to_laugh\" == \"try not to laugh\"\n",
    "    return str(s).strip().lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "\n",
    "def _match_self_row(df: pd.DataFrame, task: str):\n",
    "    \"\"\"\n",
    "    Session A: task 'video_<Emotion>' -> match df['Video Emotion']\n",
    "    Session B: task '<activity>' -> match df['Activity Name']\n",
    "    Fallback: try df['Video Name'] / df['Activity Name'] exact (normalized) match.\n",
    "    Returns pd.Series or None.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "\n",
    "    t = str(task)\n",
    "\n",
    "    # Session A: video_* tasks\n",
    "    if t.lower().startswith(\"video_\") and \"Video Emotion\" in df.columns:\n",
    "        target = _norm(t.split(\"_\", 1)[1])\n",
    "        hit = df[df[\"Video Emotion\"].astype(str).map(_norm) == target]\n",
    "        return hit.iloc[0] if len(hit) else None\n",
    "\n",
    "    # Session B: activity tasks\n",
    "    if \"Activity Name\" in df.columns:\n",
    "        target = _norm(t)\n",
    "        hit = df[df[\"Activity Name\"].astype(str).map(_norm) == target]\n",
    "        return hit.iloc[0] if len(hit) else None\n",
    "\n",
    "    # Fallback: try Video Name if present\n",
    "    if \"Video Name\" in df.columns:\n",
    "        target = _norm(t)\n",
    "        hit = df[df[\"Video Name\"].astype(str).map(_norm) == target]\n",
    "        return hit.iloc[0] if len(hit) else None\n",
    "\n",
    "    return None\n",
    "\n",
    "def _asarray(x):\n",
    "    if isinstance(x, dict):\n",
    "        for k in (\"data\",\"values\",\"samples\",\"signal\",\"x\",\"frames\"):\n",
    "            if k in x:\n",
    "                x = x[k]; break\n",
    "    return np.asarray(x)\n",
    "\n",
    "def _slice_1d_or_2d(arr, s, e):\n",
    "    arr = _asarray(arr)\n",
    "    N = arr.shape[0]\n",
    "    s = max(0, min(N, int(s)))\n",
    "    e = max(0, min(N, int(e)))\n",
    "    if e <= s:\n",
    "        return None\n",
    "    return arr[s:e]\n",
    "\n",
    "def _mean_std(arr):\n",
    "    if arr is None:\n",
    "        return None\n",
    "    a = np.asarray(arr)\n",
    "    if a.ndim == 1:\n",
    "        a = a.astype(np.float64)\n",
    "        return float(np.nanmean(a)), float(np.nanstd(a))\n",
    "    if a.ndim == 2:\n",
    "        a = a.astype(np.float64)\n",
    "        return (np.nanmean(a, axis=0), np.nanstd(a, axis=0))  # per-channel\n",
    "    return None\n",
    "\n",
    "def print_task_block(task, s, e, sensor_dict, self_row=None, fs=90.0):\n",
    "    dur_s = max(0, e - s) / fs\n",
    "    print(f\"\\n------------------------------- Task: {task} -------------------------------\")\n",
    "    print(f\"duration: {dur_s:.2f}s  (idx {s} → {e})\")\n",
    "\n",
    "    # sensors: mean/std\n",
    "    for stream, obj in sensor_dict.items():\n",
    "        # skip non-timeseries here (ET is image frames)\n",
    "        if stream == \"et\":\n",
    "            continue\n",
    "        chunk = _slice_1d_or_2d(obj, s, e)\n",
    "        ms = _mean_std(chunk)\n",
    "        if ms is None:\n",
    "            print(f\"{stream}: n/a\")\n",
    "            continue\n",
    "\n",
    "        mean, std = ms\n",
    "        if isinstance(mean, np.ndarray):  # multi-channel\n",
    "            # print compactly: mean/std per channel\n",
    "            mean_str = np.array2string(mean, precision=3, separator=\", \", max_line_width=140)\n",
    "            std_str  = np.array2string(std,  precision=3, separator=\", \", max_line_width=140)\n",
    "            print(f\"{stream}: mean={mean_str}  std={std_str}\")\n",
    "        else:\n",
    "            print(f\"{stream}: mean={mean:.4f}  std={std:.4f}\")\n",
    "\n",
    "    # self-reports\n",
    "    print(\"\\nself-reports:\")\n",
    "    if self_row is None:\n",
    "        print(\"valence: n/a\")\n",
    "        print(\"arousal: n/a\")\n",
    "        print(\"dominance: n/a\")\n",
    "        print(\"\\nemotions: n/a\")\n",
    "        return\n",
    "\n",
    "    # VAD\n",
    "    v = self_row.get(\"Valence\", None)\n",
    "    a = self_row.get(\"Arousal\", None)\n",
    "    d = self_row.get(\"Dominance\", None)\n",
    "    print(f\"valence: {v if pd.notna(v) else 'n/a'}\")\n",
    "    print(f\"arousal: {a if pd.notna(a) else 'n/a'}\")\n",
    "    print(f\"dominance: {d if pd.notna(d) else 'n/a'}\")\n",
    "\n",
    "    # discrete emotions (use the emotion columns as given)\n",
    "    print(\"\\nemotions:\")\n",
    "    for c in EMO_COLS:\n",
    "        if c in self_row.index:\n",
    "            val = self_row[c]\n",
    "            # keep original scale (often 0..1)\n",
    "            print(f\"{c.lower()}: {float(val):.1f}\" if pd.notna(val) else f\"{c.lower()}: n/a\")\n",
    "\n",
    "def print_session_summary(participant, task_times, sensor_dict, session_df, session_name, fs=90.0):\n",
    "    segs = task_times[participant]\n",
    "    shift = int(segs[\"session_A\"][0])\n",
    "\n",
    "    # decide which tasks belong to this session by intersection with [session_start, session_end]\n",
    "    sess_key = \"session_A\" if session_name.lower().endswith(\"a\") else \"session_B\"\n",
    "    if sess_key not in segs:\n",
    "        raise KeyError(f\"{sess_key} not found in task_times[{participant}]\")\n",
    "    sess_s, sess_e = map(int, segs[sess_key])\n",
    "    sess_s -= shift\n",
    "    sess_e -= shift\n",
    "\n",
    "    # build task list (exclude session_A/B)\n",
    "    tasks = []\n",
    "    for k,(s,e) in segs.items():\n",
    "        if k in (\"session_A\",\"session_B\"):\n",
    "            continue\n",
    "        s0, e0 = int(s)-shift, int(e)-shift\n",
    "        # keep tasks that overlap the chosen session window\n",
    "        if e0 > sess_s and s0 < sess_e:\n",
    "            tasks.append((k, s0, e0))\n",
    "    tasks.sort(key=lambda x: x[1])\n",
    "\n",
    "    # prep session df\n",
    "    df = session_df.copy() if session_df is not None else None\n",
    "    if df is not None:\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        for c in [\"Valence\",\"Arousal\",\"Dominance\"] + [c for c in EMO_COLS if c in df.columns]:\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if \"Video Emotion\" in df.columns:\n",
    "            df[\"Video Emotion\"] = df[\"Video Emotion\"].astype(str).str.strip()\n",
    "\n",
    "    print(f\"\\n================ {session_name} ==================\")\n",
    "\n",
    "    for task, s, e in tasks:\n",
    "        row = _match_self_row(df, task)\n",
    "        print_task_block(task, s, e, sensor_dict, self_row=row, fs=fs)\n",
    "\n",
    "\n",
    "print_session_summary(participant, task_times, sensor_dict, session_A_df, \"Session A\", fs=90.0)\n",
    "print_session_summary(participant, task_times, sensor_dict, session_B_df, \"Session B\", fs=90.0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef739be2",
   "metadata": {},
   "source": [
    "## Sensor streams visualization\n",
    "\n",
    "Important: These are unfiltered plots of the physiological data. Please filter them and zoom in accordingly to analyze them."
   ]
  },
  {
   "cell_type": "code",
   "id": "1f5d470a",
   "metadata": {},
   "source": [
    "def plot_stream(stream_dict, name, n_channels, title=None, task_times=task_times, participant=participant, alpha=0.3):\n",
    "    \"\"\"\n",
    "    Plot the first n_channels of a stream on the same axes.\n",
    "    Optionally overlays activity segments from task_times[participant] as colored backgrounds.\n",
    "    X-axis = sample index.\n",
    "    \"\"\"\n",
    "    if name not in stream_dict:\n",
    "        raise KeyError(f\"{name} not in stream_dict. Available: {list(stream_dict.keys())}\")\n",
    "\n",
    "    data = stream_dict[name]\n",
    "    if isinstance(data, dict):\n",
    "        for k in (\"data\", \"values\", \"samples\", \"signal\", \"x\"):\n",
    "            if k in data:\n",
    "                data = data[k]\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"{name} is a dict but no data key found.\")\n",
    "\n",
    "    arr = np.asarray(data)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr.reshape(-1, 1)\n",
    "    elif arr.ndim > 2:\n",
    "        arr = arr.reshape(arr.shape[0], -1)\n",
    "\n",
    "    N, C = arr.shape\n",
    "    use_c = min(n_channels, C)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "    # --- Activity backgrounds ---\n",
    "    if task_times is not None and participant is not None and participant in task_times:\n",
    "        segs = task_times[participant]\n",
    "        if \"session_A\" not in segs or segs[\"session_A\"] is None:\n",
    "            raise KeyError(f\"task_times['{participant}'] must contain 'session_A'=[start,end] to shift indices.\")\n",
    "        sessionA0 = int(segs[\"session_A\"][0])\n",
    "\n",
    "        # labels excluding sessions\n",
    "        labels = sorted([k for k in segs.keys() if k not in (\"session_A\", \"session_B\")])\n",
    "\n",
    "        cmap = plt.get_cmap(\"tab20\")\n",
    "        color_map = {lbl: cmap(i % cmap.N) for i, lbl in enumerate(labels)}\n",
    "        task_handles = [Patch(facecolor=color_map[lbl], edgecolor=\"none\", alpha=alpha, label=lbl) for lbl in labels]\n",
    "\n",
    "        # draw spans in start-time order (after shift)\n",
    "        spans = []\n",
    "        for lbl in labels:\n",
    "            s, e = segs[lbl]\n",
    "            s = int(s) - sessionA0\n",
    "            e = int(e) - sessionA0\n",
    "            spans.append((lbl, s, e))\n",
    "        spans.sort(key=lambda x: x[1])\n",
    "\n",
    "        for lbl, s, e in spans:\n",
    "            if e <= 0 or s >= N:\n",
    "                continue\n",
    "            s = max(0, s)\n",
    "            e = min(N - 1, e)\n",
    "            col = color_map[lbl]\n",
    "            ax.axvspan(s, e, color=col, alpha=alpha, linewidth=0)\n",
    "            ax.axvline(s, color=col, alpha=min(alpha * 4, 0.6), linewidth=1)\n",
    "            ax.axvline(e, color=col, alpha=min(alpha * 4, 0.6), linewidth=1)\n",
    "\n",
    "    # --- Signals ---\n",
    "    for ch in range(use_c):\n",
    "        ax.plot(arr[:, ch], label=f\"ch{ch}\")\n",
    "\n",
    "    ax.set_title(title or f\"{name} | N={N} (plotted {use_c}/{C} channels)\")\n",
    "    ax.set_xlabel(\"sample index\")\n",
    "    ax.set_ylabel(\"value\")\n",
    "\n",
    "    # Channel legend (top-right) if few channels\n",
    "    if use_c <= 12:\n",
    "        ax.legend(ncols=min(use_c, 6), fontsize=8, loc=\"upper right\")\n",
    "\n",
    "    # Task legend (right side)\n",
    "    if task_handles:\n",
    "        ax.legend(\n",
    "            handles=task_handles,\n",
    "            fontsize=7,\n",
    "            loc=\"upper left\",\n",
    "            bbox_to_anchor=(1.01, 1.0),\n",
    "            borderaxespad=0.0,\n",
    "            title=\"Tasks\",\n",
    "            title_fontsize=8,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ECG: 1 signal\n",
    "plot_stream(sensor_dict, \"ecg_90fps\", n_channels=1, title=\"ECG (1 channel)\")\n",
    "\n",
    "# EDA: 1 signal\n",
    "plot_stream(sensor_dict, \"eda_90fps\", n_channels=1, title=\"EDA (1 channel)\")\n",
    "\n",
    "# GAZE: 2 signals\n",
    "# plot_stream(sensor_dict, \"gaze_90fps\", n_channels=2, title=\"GAZE (2 channels)\")\n",
    "\n",
    "# IMU: 6 signals\n",
    "plot_stream(sensor_dict, \"imu_right_90fps\", n_channels=6, title=\"IMU (6 channels)\")\n",
    "\n",
    "# Intensity: 1 signal\n",
    "plot_stream(sensor_dict, \"intensity_90fps\", n_channels=1, title=\"Intensity (1 channel)\")\n",
    "\n",
    "# PPG EAR: 1 signal\n",
    "plot_stream(sensor_dict, \"ppg_ear_90fps\", n_channels=1, title=\"PPG EAR (1 channel)\")\n",
    "\n",
    "# PPG NOSE: 1 signal\n",
    "plot_stream(sensor_dict, \"ppg_nose_90fps\", n_channels=1, title=\"PPG NOSE (1 channel)\")\n",
    "\n",
    "# PUPILS: 2 signal\n",
    "plot_stream(sensor_dict, \"pupils_90fps\", n_channels=2, title=\"PUPILS (2 channels)\")\n",
    "\n",
    "# RR: 1 signal\n",
    "plot_stream(sensor_dict, \"rr_90fps\", n_channels=1, title=\"RR (1 channel)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a9aedcfa",
   "metadata": {},
   "source": [
    "## Eye-tracking, POV and face webcam videos visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "69bfd04a",
   "metadata": {},
   "source": [
    "\n",
    "def _sharpness_score(img_2d: np.ndarray) -> float:\n",
    "    img = img_2d.astype(np.float32)\n",
    "    gy, gx = np.gradient(img)\n",
    "    lap = np.gradient(gx, axis=1) + np.gradient(gy, axis=0)\n",
    "    return float(np.var(lap))\n",
    "\n",
    "def _read_frame_by_index(cap: cv2.VideoCapture, frame_idx: int):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_idx))\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        return None\n",
    "    return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def show_et_with_videos_by_task(\n",
    "    stream_dict: dict,\n",
    "    task_times: dict,\n",
    "    participant: str,\n",
    "    pov_path: str,\n",
    "    webcam_path: str,\n",
    "    et_fps: float = 90.0,\n",
    "    pov_fps: float = 10.0,\n",
    "    webcam_fps: float = 60.0,\n",
    "    exclude=(\"session_A\", \"session_B\"),\n",
    "    stride: int = 20,\n",
    "    figsize_row: float = 2.8,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each task:\n",
    "      - pick a representative ET frame (sharpest within segment)\n",
    "      - show ET + closest POV frame + closest Webcam frame side-by-side\n",
    "\n",
    "    Index mapping:\n",
    "      t = et_idx / et_fps + offset_s\n",
    "      pov_frame   = round(t * pov_fps)\n",
    "      webcam_frame= round(t * webcam_fps)\n",
    "\n",
    "    Also applies shift task indices by session_A[0].\n",
    "    \"\"\"\n",
    "\n",
    "    # --- get ET frames from stream_dict['et'] ---\n",
    "    if \"et\" not in stream_dict:\n",
    "        raise KeyError(f\"'et' not in stream_dict. Available: {list(stream_dict.keys())}\")\n",
    "\n",
    "    et = stream_dict[\"et\"]\n",
    "    if isinstance(et, dict):\n",
    "        for k in (\"frames\", \"data\", \"values\", \"samples\", \"x\"):\n",
    "            if k in et:\n",
    "                et = et[k]\n",
    "                break\n",
    "\n",
    "    frames = np.asarray(et)\n",
    "    if frames.ndim != 3:\n",
    "        raise ValueError(f\"Expected et frames with shape (N,H,W), got {frames.shape}\")\n",
    "\n",
    "    segs = task_times[participant]\n",
    "    sessionA0 = int(segs[\"session_A\"][0])\n",
    "\n",
    "    tasks = [k for k in segs.keys() if k not in exclude]\n",
    "    tasks.sort(key=lambda k: segs[k][0])\n",
    "\n",
    "    # open videos once\n",
    "    cap_pov = cv2.VideoCapture(pov_path)\n",
    "    cap_web = cv2.VideoCapture(webcam_path)\n",
    "    if not cap_pov.isOpened():\n",
    "        raise RuntimeError(f\"Could not open POV video: {pov_path}\")\n",
    "    if not cap_web.isOpened():\n",
    "        raise RuntimeError(f\"Could not open webcam video: {webcam_path}\")\n",
    "\n",
    "    pov_n = int(cap_pov.get(cv2.CAP_PROP_FRAME_COUNT)) or None\n",
    "    web_n = int(cap_web.get(cv2.CAP_PROP_FRAME_COUNT)) or None\n",
    "\n",
    "    picks = []\n",
    "    N = frames.shape[0]\n",
    "\n",
    "    for t in tasks:\n",
    "        s, e = map(int, segs[t])\n",
    "        s = s - sessionA0\n",
    "        e = e - sessionA0\n",
    "        s = max(0, s)\n",
    "        e = min(N - 1, e)\n",
    "        if e <= s:\n",
    "            continue\n",
    "\n",
    "        best_i, best_sc = s, -1.0\n",
    "        for i in range(s, e + 1, stride):\n",
    "            sc = _sharpness_score(frames[i])\n",
    "            if sc > best_sc:\n",
    "                best_sc, best_i = sc, i\n",
    "\n",
    "        # map ET index -> video frame indices via time\n",
    "        t_sec = (best_i / et_fps)\n",
    "        pov_idx = int(round(t_sec * pov_fps))\n",
    "        web_idx = int(round(t_sec * webcam_fps))\n",
    "\n",
    "        # clamp (if frame counts are available)\n",
    "        if pov_n is not None:\n",
    "            pov_idx = max(0, min(pov_n - 1, pov_idx))\n",
    "        else:\n",
    "            pov_idx = max(0, pov_idx)\n",
    "\n",
    "        if web_n is not None:\n",
    "            web_idx = max(0, min(web_n - 1, web_idx))\n",
    "        else:\n",
    "            web_idx = max(0, web_idx)\n",
    "\n",
    "        picks.append((t, best_i, pov_idx, web_idx))\n",
    "\n",
    "    # plot: one row per task, 3 columns (ET / POV / Webcam)\n",
    "    rows = len(picks)\n",
    "    fig, axes = plt.subplots(rows, 3, figsize=(12, max(2.5, rows * figsize_row)))\n",
    "\n",
    "    if rows == 1:\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    for r, (t, et_i, pov_i, web_i) in enumerate(picks):\n",
    "        et_img = frames[et_i]\n",
    "        pov_img = _read_frame_by_index(cap_pov, pov_i)\n",
    "        web_img = _read_frame_by_index(cap_web, web_i)\n",
    "\n",
    "        ax0, ax1, ax2 = axes[r]\n",
    "\n",
    "        ax0.imshow(et_img, cmap=\"gray\", interpolation=\"nearest\")\n",
    "        ax0.set_title(f\"{t}\\nET idx={et_i}\", fontsize=9)\n",
    "        ax0.axis(\"off\")\n",
    "\n",
    "        if pov_img is None:\n",
    "            ax1.text(0.5, 0.5, \"POV read failed\", ha=\"center\", va=\"center\")\n",
    "        else:\n",
    "            ax1.imshow(pov_img)\n",
    "        ax1.set_title(f\"POV idx={pov_i}\", fontsize=9)\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "        if web_img is None:\n",
    "            ax2.text(0.5, 0.5, \"Webcam read failed\", ha=\"center\", va=\"center\")\n",
    "        else:\n",
    "            ax2.imshow(web_img)\n",
    "        ax2.set_title(f\"Webcam idx={web_i}\", fontsize=9)\n",
    "        ax2.axis(\"off\")\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.10, wspace=0.02)\n",
    "    plt.show()\n",
    "\n",
    "    cap_pov.release()\n",
    "    cap_web.release()\n",
    "\n",
    "show_et_with_videos_by_task(\n",
    "    sensor_dict, task_times, participant=participant,\n",
    "    pov_path=f\"{original_data_path}/{participant}/pov.mp4\", webcam_path=f\"{original_data_path}/{participant}/webcam.mp4\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5d19a33d",
   "metadata": {},
   "source": [
    "## Self-reports Visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "4bf903f2",
   "metadata": {},
   "source": [
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "EMO_COLS = [\"Amused\",\"Content\",\"Excited\",\"Awe\",\"Neutral\",\"Fear\",\"Sad\",\"Disgust\",\"Anger\"]\n",
    "VAD_COLS = [\"Valence\",\"Arousal\",\"Dominance\"]\n",
    "BIG5 = [\"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Negative Emotionality\", \"Open-Mindedness\"]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return str(s).strip().lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "\n",
    "def _to_float(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() == \"nan\":\n",
    "        return np.nan\n",
    "    return float(s.replace(\",\", \".\"))\n",
    "\n",
    "# ----------------------------\n",
    "# Task list from task_times (shift by session_A[0], skip session_A/B)\n",
    "# ----------------------------\n",
    "def get_session_tasks(task_times, participant, session_key):\n",
    "    segs = task_times[participant]\n",
    "    shift = int(segs[\"session_A\"][0])\n",
    "\n",
    "    sess_s, sess_e = map(int, segs[session_key])\n",
    "    sess_s -= shift\n",
    "    sess_e -= shift\n",
    "\n",
    "    tasks = []\n",
    "    for k, (s, e) in segs.items():\n",
    "        if k in (\"session_A\", \"session_B\"):\n",
    "            continue\n",
    "        s0, e0 = int(s) - shift, int(e) - shift\n",
    "        if e0 > sess_s and s0 < sess_e:\n",
    "            tasks.append((k, s0, e0))\n",
    "    tasks.sort(key=lambda x: x[1])\n",
    "    return tasks  # list of (task, start, end)\n",
    "\n",
    "# ----------------------------\n",
    "# Self-report aggregation\n",
    "# ----------------------------\n",
    "def prepare_self_df(df):\n",
    "    if df is None:\n",
    "        return None\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    for c in VAD_COLS + [c for c in EMO_COLS if c in df.columns]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    for c in (\"Video Emotion\", \"Video Name\", \"Activity Name\"):\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "            df[f\"_{c}_norm\"] = df[c].map(_norm)\n",
    "\n",
    "    return df\n",
    "\n",
    "def aggregate_self_report_for_task(df, task):\n",
    "    \"\"\"\n",
    "    Returns dict with keys VAD + EMO_COLS aggregated for the task.\n",
    "    - Session A: task 'video_<Emotion>' matches Video Emotion (can aggregate multiple rows).\n",
    "    - Session B: task '<activity>' matches Activity Name (can aggregate duplicates).\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "\n",
    "    task = str(task)\n",
    "\n",
    "    if task.lower().startswith(\"video_\") and \"_Video Emotion_norm\" in df.columns:\n",
    "        target = _norm(task.split(\"_\", 1)[1])\n",
    "        sub = df[df[\"_Video Emotion_norm\"] == target]\n",
    "    elif \"_Activity Name_norm\" in df.columns:\n",
    "        target = _norm(task)\n",
    "        sub = df[df[\"_Activity Name_norm\"] == target]\n",
    "    elif \"_Video Name_norm\" in df.columns:\n",
    "        target = _norm(task)\n",
    "        sub = df[df[\"_Video Name_norm\"] == target]\n",
    "    else:\n",
    "        sub = df.iloc[0:0]\n",
    "\n",
    "    if sub.empty:\n",
    "        return None\n",
    "\n",
    "    out = {}\n",
    "    for c in VAD_COLS:\n",
    "        out[c] = float(sub[c].mean()) if c in sub.columns else np.nan\n",
    "    for c in EMO_COLS:\n",
    "        out[c] = float(sub[c].mean()) if c in sub.columns else np.nan\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Plotting helpers\n",
    "# ----------------------------\n",
    "def radar_plot(values, title, labels, ax=None, rlim=None, fill_alpha=0.12):\n",
    "    \"\"\"\n",
    "    Radar plot that can draw into an existing polar axis (ax=...) or create a new fig if ax is None.\n",
    "    values: list/array numeric (len == len(labels))\n",
    "    \"\"\"\n",
    "    values = np.array(values, dtype=float)\n",
    "    values = np.nan_to_num(values, nan=0.0)\n",
    "\n",
    "    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n",
    "    values = np.r_[values, values[0]]\n",
    "    angles = np.r_[angles, angles[0]]\n",
    "\n",
    "    created = False\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5.4, 5.4), subplot_kw={\"polar\": True})\n",
    "        created = True\n",
    "\n",
    "    ax.plot(angles, values)\n",
    "    ax.fill(angles, values, alpha=fill_alpha)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels, fontsize=10)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "    if rlim is not None:\n",
    "        ax.set_ylim(*rlim)\n",
    "\n",
    "    if created:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_task_self_report(task, agg):\n",
    "    \"\"\"One task: spider for emotions + bar for V/A/D.\"\"\"\n",
    "    if agg is None:\n",
    "        print(f\"[WARN] No self-report found for task: {task}\")\n",
    "        return\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1, polar=True)\n",
    "    emo_vals = [agg.get(c, np.nan) for c in EMO_COLS]\n",
    "    radar_plot(emo_vals, f\"{task} - Discrete Emotions\", EMO_COLS, ax=ax1, rlim=(0, 1))\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    vad_vals = [agg.get(c, np.nan) for c in VAD_COLS]\n",
    "    ax2.bar(VAD_COLS, [0 if np.isnan(v) else v for v in vad_vals])\n",
    "    ax2.set_ylim(1, 7)\n",
    "    ax2.set_title(f\"{task} - Continuous Affect\", fontsize=11)\n",
    "    ax2.set_ylabel(\"rating\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_session_self_reports(session_name, session_df, tasks):\n",
    "    \"\"\"\n",
    "    tasks: list of (task, start, end) - we only use task name for self-report viz.\n",
    "    \"\"\"\n",
    "    df = prepare_self_df(session_df)\n",
    "    print(f\"\\n================ {session_name} — Self-report visualizations ================\")\n",
    "    for task, _, _ in tasks:\n",
    "        agg = aggregate_self_report_for_task(df, task)\n",
    "        plot_task_self_report(task, agg)\n",
    "\n",
    "# ----------------------------\n",
    "# Personality\n",
    "# ----------------------------\n",
    "def personality_tables_from_wide(personality_df):\n",
    "    df = personality_df.copy()\n",
    "\n",
    "    # participant IDs are in row 0, from col 1 onward\n",
    "    id_row = df.iloc[0]\n",
    "    value_cols = df.columns[1:]\n",
    "    participant_ids = [str(id_row[c]).strip() for c in value_cols]\n",
    "    col_map = {col: pid for col, pid in zip(value_cols, participant_ids)}\n",
    "\n",
    "    # find separator row \"T-score\" in first column\n",
    "    sep_idx = df.index[df.iloc[:, 0].astype(str).str.strip().str.lower() == \"t-score\"]\n",
    "    if len(sep_idx) == 0:\n",
    "        raise ValueError(\"Could not find the 'T-score' separator row.\")\n",
    "    sep = int(sep_idx[0])\n",
    "\n",
    "    def build_section(start, end):\n",
    "        block = df.iloc[start:end].copy()\n",
    "        trait = block.iloc[:, 0].astype(str).str.strip()\n",
    "        block = block[trait.notna() & (trait != \"\")].copy()\n",
    "        block.iloc[:, 0] = block.iloc[:, 0].astype(str).str.strip()\n",
    "\n",
    "        vals = block.loc[:, value_cols].rename(columns=col_map)\n",
    "        vals = vals.map(_to_float)\n",
    "        vals.index = block.iloc[:, 0].values\n",
    "        return vals\n",
    "\n",
    "    mean_df = build_section(start=1, end=sep)\n",
    "    tscore_df = build_section(start=sep+1, end=len(df))\n",
    "    return mean_df, tscore_df\n",
    "\n",
    "def plot_personality_from_wide(personality_df, participant=\"005\", use=\"T-score\"):\n",
    "    mean_df, t_df = personality_tables_from_wide(personality_df)\n",
    "    pid = str(participant).strip()\n",
    "    use_l = use.strip().lower()\n",
    "\n",
    "    if use_l.startswith(\"t\"):\n",
    "        table = t_df\n",
    "        rlim = (20, 80)\n",
    "        name = \"T-score\"\n",
    "    else:\n",
    "        table = mean_df\n",
    "        rlim = None\n",
    "        name = \"Mean\"\n",
    "\n",
    "    if pid not in table.columns:\n",
    "        raise KeyError(f\"Participant '{pid}' not found. Example columns: {list(table.columns)[:10]}\")\n",
    "\n",
    "    vals = [table.loc[t, pid] if t in table.index else np.nan for t in BIG5]\n",
    "    radar_plot(vals, f\"Personality ({name}) — {pid}\", BIG5, rlim=rlim)\n",
    "\n",
    "\n",
    "# Personality\n",
    "plot_personality_from_wide(personality_df, participant=participant, use=\"T-score\")\n",
    "plot_personality_from_wide(personality_df, participant=participant, use=\"Mean\")\n",
    "\n",
    "# Self-reports by task\n",
    "tasks_A = get_session_tasks(task_times, participant, \"session_A\")\n",
    "tasks_B = get_session_tasks(task_times, participant, \"session_B\")\n",
    "\n",
    "plot_session_self_reports(\"Session A\", session_A_df, tasks_A)\n",
    "plot_session_self_reports(\"Session B\", session_B_df, tasks_B)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f899aa1c53205284",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocrl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
